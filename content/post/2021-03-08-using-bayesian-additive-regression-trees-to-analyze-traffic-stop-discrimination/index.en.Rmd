---
title: Using Bayesian Additive Regression Trees to Analyze Traffic Stop Bias
author: ''
date: '2021-03-08'
slug: using-bayesian-additive-regression-trees-to-analyze-traffic-stop-discrimination
categories:
  - bayesian analysis
  - traffic stops
  - Bayesian Additive Regression Trees
tags: []
subtitle: ''
summary: ''
authors: []
lastmod: '2021-03-08T19:22:36-05:00'
featured: no
image:
  caption: '<span>Photo by <a href="https://unsplash.com/@gerandeklerk?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText">Geran de Klerk</a> on <a href="https://unsplash.com/s/photos/forest?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText">Unsplash</a></span>'
  focal_point: ''
  preview_only: no
projects: []
links:
- icon: github
  icon_pack: fab
  name: Github
  url: https://github.com/shanejorr/shanes-site/blob/main/content/post/2021-03-08-using-bayesian-additive-regression-trees-to-analyze-traffic-stop-discrimination/index.en.Rmd
draft: yes
---

```{r setup, echo = F}
knitr::opts_chunk$set(echo = T,
                      message = F,
                      warning = F,
                      error = F,
                      eval = T)
```

Bayesian Additive Regression Trees (BART) are a Bayesian version of random forests. They're flexible, non-parametric, and supply a full posterior distribution on outcomes. They have all the benefits of both Bayesian modeling and non-parametric machine learning modeling in one algorithm.

In this post, I practice with BART models using traffic stop data as the backdrop. We'll apply BART to the veil of darkness test and see if we can uncover racial disparities on traffic stops.

## Why BART?

We're not going into the technical details of BART here. Instead, check out the following resources To understand the algorithm's guts:

-   [BART: Bayesian additive regression trees](https://arxiv.org/abs/0806.3286) - The paper that introduced the world to BART.^[Chipman, H. A., George, E. I., & McCulloch, R. E. (2010). BART: Bayesian additive regression trees. The Annals of Applied Statistics, 4(1), 266-298.]
-   [Bayesian Nonparametric Modeling for Causal Inference](https://www.tandfonline.com/doi/abs/10.1198/jcgs.2010.08162) - Jennifer Hill's paper that brought us BART for causal inference.^[Hill, J. L. (2011). Bayesian nonparametric modeling for causal inference. Journal of Computational and Graphical Statistics, 20(1), 217-240.]
-   [Causal Inference at the Intersection of Statistics and Machine Learning](https://www.youtube.com/watch?v=mJVTg8PWjW4) - Video presentation by Jennifer Hill on BART fo causal inference. The video isn't great quality, but it's an easy to follow introduction to BART.
-   [Introduction to Bayesian Additive Regression Trees (BART) for Causal Inference](https://www.youtube.com/watch?v=9d5-3_7u5a4) - Another good video introduction to BART for causal inference.

In general terms, however, I view the pros of BART as falling into two categories: (1) reasons it's better than parametric Bayesian models such as linear or logistic regression, and (2) reasons it's better than other non-parametric methods like random forests. Let's look at these two categories.

### Why BART is (often) better than parametric Bayesian methods

Compared to parametric models, BART lets you easily and automatically discover a prediction model's best functional form. The best, at least, in terms of partitioning the feature space as trees. In a way, it's plug and play. 

There are a few reasons why this brings benefits over parametric Bayesian models. First, no more worrying about the assumption of linearity between the predictors and the outcome. No more fishing around with splines or polynomials just in case the relationship is nonlinear. If it's nonlinear, the algorithm will create a nonlinear fit without you intervening.

For causal inference, the algorithm automatically incorporates interactions to the degree they appear in the data. No more trying multiple models, some with and some without interactions, to see if there are interaction effects. You get them for free.

Finally, variable selection is simplified. Predictors that do not predict the outcome will simply not find their way into any trees. Humans don't have to identify and exclude them, the model identifies and excludes them.

### Why BART is (often) better than other non-parametric methods

There's one answer here: you get the full posterior distribution of your outcome just like with any Bayesian model. This is crucial with causal inference as it let's you map the uncertainty of your treatment effects. Black-box machine learning models don't do this. They give you one answer and tell you little about uncertainty in the answer.

```{r importLibraries}
library(glue)
library(vroom)
library(modelr)
library(tidycensus)
library(lubridate)
library(highcharter)
library(scales)
library(widgetframe) 
library(suncalc)
library(tigris)
library(sf)
library(BART)
library(rstanarm)
library(tidybayes)
library(lme4)
library(gt)
library(tidyverse)
```

```{r customFunctions}
# custom functions --------------------------

# create ggplot theme for this post
post_theme <- theme_minimal() +
  theme(legend.position = 'bottom',
        plot.title = element_text(size = 11),
        plot.subtitle = element_text(size = 10), 
        axis.title = element_text(size = 10))

theme_set(post_theme)

# standard credits to all to all high charts plots

# credits to be used on all plots
add_credits <- function(hc) {
  hc %>%
    hc_credits(
      enabled = TRUE,
      text = "Data source: The Stanford Open Policing Project | US Census Bureau Pop. Estimates",
      href = FALSE
  )
}

scatter_stops_per <- function(df, second_race, plot_title, y_title) {
  
  # scatter plot of stops per 100 residents for white and black / hispanic drivers
  # second_race is the race to compare with white
  
  # max axis point
  max_point <- 45
  
  # find out if white or minority group is higher, used for colors above or below line
  df[['diff']] <- ifelse(df[['white']] < df[[second_race]], 1, 0)
  
  # assign color to each row
  color_palette <- c('#4e79a7', '#f28e2b')
  df[['colors']] <- colorize(df[['diff']], color_palette)
  
  # tooltip
  scatter_tooltip <- "<b>{point.county_name}</b><br>
                      Stops per 100 residents (Black): <b>{point.black}</b><br>
                      Stops per 100 residents (White): <b>{point.white}</b><br>
                      Stops per 100 residents (Hispanic): <b>{point.hispanic}</b><br>
                      County population (2013): <b>{point.value_tooltip}</b><br>"
  
  df %>%
    hchart(
      "bubble", 
      hcaes(x = 'white', y = .data[[second_race]], size = value, color = colors)
    ) %>%
    hc_add_series(name = 'line',
                  data = data.frame(
                    x = seq(1, max_point, 1),
                    y = seq(1, max_point, 1),
                    type = 'line',
                    marker = list(enabled = FALSE)
                  )) %>%
    # remove tooltip for line
    hc_plotOptions(line = list(enableMouseTracking = FALSE),
                   bubble = list(opacity = .85,
                                 minSize = "1%", maxSize = "7%")) %>%
    hc_xAxis(min = 1, max = max_point, type = 'logarithmic',
             title = list(text = 'Stops per 100 residents for White drivers')) %>% 
    hc_yAxis(min = 1, max = max_point, type = 'logarithmic',
             title = list(text = y_title)) %>%
    hc_tooltip(
        headerFormat = "",
        pointFormat = scatter_tooltip
    ) %>%
    hc_title(text = plot_title)
}

# extract time from date time
time_to_minute <- function(time) {
  # minutes since midnight
  hour(time) * 60 + minute(time)
}

minute_to_time <- function(minutes_since_midnight) {
  
  # convert minutes created with time_to_minute back to time (hour, minute)

  hour_with_decimal <- minutes_since_midnight/60
  
  hour_of_day <- floor(hour_with_decimal)
  
  decimal_from_hour <- hour_with_decimal - hour_of_day
  
  minute_of_hour <- signif(decimal_from_hour * 60, 1)
  
  time_of_day <- lubridate::hms(glue("{hour_of_day}:{minute_of_hour}:00"))
  
  return(time_of_day)
  
}

create_train_test_sets <- function(split_partition, predictor_variables, train_or_test = 'train') {
  
  # create train and testing datasets
  
  if (train_or_test == 'train') {
    
    pred_df <- training(split_partition)
    
  } else if (train_or_test == 'test') {
    
    pred_df <- testing(split_partition)
    
  } else {
    stop("train_or_test must be either 'train' or 'test'")
  }
  
  pred_df %>%
      ungroup() %>%
      # only keep predictors (required for BART)
      select(!!predictor_variables) %>%
      as.data.frame()
  
}

get_hdi <- function(df, hdi_colname) {

  # calcualte the highest density interval and place results in a data frame
  
  column_to_calc_hdi <- df[hdi_colname]
  
  median_hdci(column_to_calc_hdi, .width = .9) %>%
    mutate(county_time = !!hdi_colname,
           county_time = str_remove(county_time, 'Yes_')) %>%
    select(county_time, estimate = 1, .lower, .upper)
}
```

```{r importData, cache = T}
# downlaod traffic stop data --------------------------

# download entire state traffic stop dataset
nc_stops <- vroom('https://shane-datasets.nyc3.digitaloceanspaces.com/traffic-stop/nc/nc_statewide_2020_04_01.csv.gz',
                  col_select = -contains('raw')) #%>%
  #sample_frac(size = .2)

# get 2010 - 2013 5 year and aggregate
pop_years <- seq(2009, 2013, 1)

n_years <- length(pop_years)

nc_stops <- nc_stops %>%
  mutate(year = year(date)) %>%
  # only keep between years 2010 and 2013 
  # 2013 is final full year of stop data
  filter(year %in% !!pop_years)
```

## Comparing Stops Per 100 Residents Among Racial Groups

Prior to modeling racial bias with the veil of darkness test, let's take a look at the data. The traffic stop data set comes from [The Stanford Open Policing Project](https://openpolicing.stanford.edu/) and is the same data used in [the last post](https://www.shaneorr.io/post/exploratory-analysis-of-north-carolina-traffic-stop-data/).

```{r importCensus, cache = T}
# racial populations by county from census ----------------

# recode integers for dates to years
recode_date <- c(
  `3` = 2010,
  `4` = 2011,
  `5` = 2012,
  `6` = 2013,
  `7` = 2014,
  `8` = 2015
)

# get 2013 overall county populations
county_pop <- get_estimates(geography = "county", state = 'NC',
                            product = 'population', 
                            time_series = T,
                            year = 2018) %>%
  # only keep 2013 and only keep population (not density)
  filter(DATE == 6,
         variable == 'POP') %>%
  mutate(NAME = str_remove_all(NAME, ' County, North Carolina')) %>%
  select(NAME, value)

# import population data by county and race
county_pop_race <- get_estimates(geography = "county", state = 'NC',
                                product = 'characteristics', 
                                breakdown = c('RACE', 'HISP'),
                                breakdown_labels = T,
                                time_series = T,
                                year = 2018) 

# calculate aggregate percentage of population by race
county_pop_agg <- county_pop_race %>%
  # only keep 2010 - 2013
  filter(between(DATE, 3, 6)) %>%
  # aggregate population by race for each county
  group_by(GEOID, NAME, RACE, HISP) %>%
  summarize(agg_pop = sum(value)) %>%
  ungroup() %>%
  filter(HISP != 'Both Hispanic Origins') %>%
  # make the All Races  race that is Hisp the Hisp race
  mutate(RACE = ifelse(RACE == 'All races' & HISP == 'Hispanic', 'Hispanic', RACE)) %>%
  # only need white, black, and hispanic
  filter(RACE %in% c('White alone', 'Black alone', 'Hispanic'),
         # do not need hispanic breakdown by race
         RACE == 'Hispanic' | HISP == 'Non-Hispanic') %>%
  # remove 'alone' phrase from race
  mutate(RACE = str_remove_all(RACE, ' alone'),
         # make lower case and remove latino to match traffic stops
         RACE = str_to_lower(RACE),
         # remove string so that it matches with traffic stop data
         NAME = str_remove_all(NAME, ' County, North Carolina')) %>%
  # don't need the hispanic column anymore
  select(-HISP)
```

```{r stopsByRace}
# calculate stops per race --------------

# save list of stop reasons to be uses in text
stop_reasons <- unique(nc_stops$reason_for_stop)

stops_by_race <- nc_stops %>%
  # calculate number of stops by race (numerator)
  group_by(county_name, subject_race) %>%
  summarize(num_stops_race = n()) %>%
  # calculate percentage of stops by race
  ungroup() %>%
  # only need three races; not enough data for others
  filter(subject_race %in% c('black', 'white', 'hispanic')) %>%
  # only keep counties with a minimum of 100 stops for each race
  group_by(county_name) %>%
  mutate(min_num = min(num_stops_race)) %>%
  ungroup() %>%
  filter(min_num >= 500) %>%
  drop_na(county_name) %>%
  mutate(county_name = str_remove_all(county_name, ' County'),
         # correct spelling mistake
         county_name = str_replace(county_name, 'Tyrell', 'Tyrrell')) %>%
  drop_na(county_name) %>%
  # combine stops by race percentages and racial population percentages
  left_join(county_pop_agg, 
            by = c('county_name' = 'NAME', 'subject_race' = 'RACE')) %>%
  # number of stops per 100 people
  mutate(stops_per = round(num_stops_race / (agg_pop / 100), 2)) %>%
  # convert to wide form where each race is in a different column
  # needed for plotting
  pivot_wider(id_cols = 'county_name', 
              names_from = 'subject_race',
              values_from = "stops_per") %>%
  # combine county populations
  left_join(county_pop,
            by = c('county_name' = 'NAME')) %>%
  mutate(value_tooltip = number(value, accuracy = 1, big.mark = ',')) %>%
  # Alleghany is odd outlier, so remove
  filter(!county_name %in% c('Alleghany'))

```

One way to show racial differences in traffic stops is to compare stops per 100 residents between racial groups. The graph below shows this comparison for all North Carolina counties. The left plot compares Black and White drivers, while the right plot highlights Hispanic and White drivers. Each point is a county. The diagonal line is the trend we would expect if drivers of different races have the same number of stops per 100 residents.

For almost all counties in North Carolina, Black drivers are stopped at a higher rate per 100 residents than White drivers. This is shown by almost all of the points lying above the diagonal line for the Black / White comparison chart. The Hispanic and White rates, however, are similar as shown by the dots in the Hispanic / White graph falling along the diagonal line.

```{r plotStopsRace, fig.cap = "More Black drivers are stopped per 100 residents than White drivers. Hispanic and White drivers, however, have similair rates."}

# scatter plot of race and perc. stops compared to overall perc. pop ----------

bw_stops_title <- 'Stops Per 100 Residents for Black and White Drivers'
bw_y_title <- 'Stops per 100 residents for Black drivers'
bw_scatter_plot <- scatter_stops_per(stops_by_race, 'black', bw_stops_title, bw_y_title)

hw_stops_title <- 'Stops Per 100 Residents for Hispanic and White Drivers'
hw_y_title <- 'Stops per 100 residents for Hispanic drivers'
hw_scatter_plot <- scatter_stops_per(stops_by_race, 'hispanic', hw_stops_title, hw_y_title) %>%
  add_credits()

hw_grid(
  bw_scatter_plot, hw_scatter_plot,
  ncol = 2, rowheight = 300
)
```

Of course, stopping our analysis here is unwarranted. First, this does not account for how often different racial groups drive. It's comparing stops per 100 residents, not stops per 100 miles driven. Second, we have not accounted for differences in policing patterns. In general, police patrol Black neighborhoods more frequently than White neighborhoods. Therefore, the greater police presence could account for the differences in stops per 100 residents. Although, it's no vindication of police to explain away the differences in stops per 100 residents by saying that their neighborhoods are more heavily patrolled. This simply kicks the bias up a step to the level of the deployment of policing assets. Finally, drivers within different racial groups may just drive differently.

## Veil of darkenss test

The veil of darkness test attempts to overcome these differences. The test has its own weaknesses and should only be seen as additional evidence. It can provide evidence for or against bias in police stops, but should never be taken as the final say.

### What is the veil of darkness test?

Here's how [a group of scholars](https://www.nature.com/articles/s41562-020-0858-1) explained the test:

>[The] method starts from the idea that officers who engage in racial profiling are less able to identify a driver’s race after dark than during the day. As a result, if officers are discriminating against black drivers—all else being equal—one would expect black drivers to comprise a smaller share of stopped drivers at night, when a veil-of-darkness masks their race. To account for patterns of driving and police deployment that may vary throughout the day, the test leverages the fact that the sun sets at different times during the year.^[Pierson, E., Simoiu, C., Overgoor, J., Corbett-Davies, S., Jenson, D., Shoemaker, A., & Goel, S. (2020). A large-scale analysis of racial disparities in police stops across the United States.() Nature human behaviour, 4(7), 736-745.]

For example, let's say it is dark at 7 p.m. for a third of the year, light for a third of the year, and kind-of dark for a third of the year. We want to compare the percentage of all drivers pulled over are Black and when it's light at 7 p.m. against the percentage of all drivers are pulled over who are Black when it's dark at 7 p.m. If a larger percentage are pulled over when it's light we have some evidence of racial bias.  

This reduces to a prediction problem. For each traffic stop, we want to predict the driver's race given the time of the stop, the location (county), whether it's dark out, and other possible covariates. Then, we'll turn around and predict every driver's race. If the probability that a driver is Black is higher during the day than at night - holding time, location, and other covariates constant - then we have evidence of racial bias. 

Researchers generally apply logistic regression to this problem. And that's a great tool. But, we're here to learn BART, so that what we're applying. Plus, BART provides all the benefits over parametric models that we mentioned in the beginning. 

```{r minutesToDuskDataset}
# create dataset for veil of darkness test -----------------------

# get centroid of every county

# get county shapefiles and centroids
nc_counties <- counties(state = 'NC', cb = T)

# find centroid of each county by using it's boundaries
nc_counties$lon<-st_coordinates(st_centroid(nc_counties))[,1]
nc_counties$lat<-st_coordinates(st_centroid(nc_counties))[,2]

# drop shapefile column
nc_counties$geometry <- NULL

nc_counties <- nc_counties %>%
  ungroup() %>%
  select(GEOID, NAME, lon, lat)

# get sunlight times for each county

# extend dataset to include a county row for each day
# neede because function to calcualte sunset requires date column in dataframe
days_seq <- seq(as.Date(min(nc_stops$date)), as.Date(max(nc_stops$date)), "days")

nc_counties_sunset <- map_df(days_seq, function(day) {
  nc_counties %>%
    mutate(date = !!day) %>%
    getSunlightTimes(
      data = .,
      keep = c("sunset", "dusk"), 
      tz = 'EST'
    )
}) %>%
  mutate(
    sunset_minute = time_to_minute(sunset),
    dusk_minute = time_to_minute(dusk),
    date = ymd(str_sub(date, 1, 10))
) %>% 
  # join county names
  left_join(nc_counties, by = c('lon', 'lat'))

# only keep the 20 most populated counties
most_pop_counties <- county_pop %>%
 arrange(desc(value)) %>%
 head(20) %>%
 .[[1]]

# merge dusk times with nc stops dataset
nc_stops_veil <- nc_stops %>%
  drop_na(time) %>%
  mutate(county_name = str_remove_all(county_name, ' County')) %>%
  # only keep the 20 most populated counties
  filter(county_name %in% !!most_pop_counties) %>%
  # add sunset and dusk times
  left_join(nc_counties_sunset, 
            by = c('county_name' = 'NAME', 'date')) %>%
  # convert date times to integer minutes from midnight
  mutate(
    minute = time_to_minute(time),
    minutes_after_dark = minute - dusk_minute,
    is_dark = minute > dusk_minute
  ) %>%
  ungroup() %>%
  group_by(county_name) %>%
  # find the min and max dusk times for each county
  mutate(
    min_dusk_minute = min(dusk_minute),
    max_dusk_minute = max(dusk_minute),
    is_black = subject_race == "black"
  ) %>% 
  filter(
    # Filter to get only the intertwilight period
    minute >= min_dusk_minute,
    minute <= max_dusk_minute,
    # Remove ambigous period between sunset and dusk
    !(minute > sunset_minute & minute < dusk_minute),
    # Compare only white and black drivers
    subject_race %in% c("black", "white")
  ) %>%
  select(date, time, sunset, dusk, contains('minute'), is_dark, everything()) %>%
  drop_na(is_black, is_dark, minute, subject_age, subject_sex, county_name) %>%
  mutate(county_name = factor(county_name),
         year = as.factor(year))

# remove items to save RAM
rm(county_pop_race)
rm(nc_stops)
rm(nc_counties_sunset)
```

```{r modelDatasets}
# create datasets for modeling and prediction -------------------

# variables needed for modeling
vars_for_model <- c('minute', 'subject_age', 'subject_sex', 'county_name', 'is_dark', 'year', 'is_black')

model_df <- nc_stops_veil %>% 
  ungroup() %>%
  select(!!vars_for_model) %>%
  drop_na() %>%
  # BART requires categoricals with more than one level to be factors
  mutate(is_dark = ifelse(is_dark, 'Yes', 'No')) %>%
  mutate(across(c('subject_sex', 'county_name', 'is_dark', 'year'), ~as.factor(.)))

response_train <- as.numeric(model_df$is_black)

predictor_df <- model_df %>% 
  select(-is_black) %>%
  as.data.frame()

# dataset to create potential outcomes
# compare differences in probability of getting stops between Black and White drivers
# for each county in 10 minute increments
potential_outcome_data <- model_df %>%
  data_grid(
    minute = seq_range(minute, by = 10, pretty = T),
    subject_age = 20,
    subject_sex = subject_sex,
    county_name = county_name,
    is_dark = is_dark,
    year = year
  ) %>%
  filter(subject_sex == 'male',
         year == '2013') %>%
  arrange(is_dark, county_name, year, minute) %>%
  as.data.frame()
```


```{r bartMod, cache = T, eval = FALSE}
# run BART model --------------------------------
bart_mod <- mc.pbart(
  x.train = predictor_df, 
  y.train = response_train, 
  x.test = potential_outcome_data, 
  keeptrainfits = F,
  ndpost = 200, 
  mc.cores = 8,
  cont = FALSE,
  seed = 99
)
write_rds(bart_mod, 'bart_mod.rds')
```

```{r modPredictions, cache = TRUE}
bart_mod <- read_rds('bart_mod.rds')

# predictions from test set that was added to model
post_predictions <- as.data.frame(bart_mod$prob.test)
 
# add decriptive column names to posterior predictions signifying what they represent
potential_outcome_data <- potential_outcome_data %>%
  mutate(descriptive_name = glue("{is_dark}_{county_name}_{minute}"))

colnames(post_predictions) <- potential_outcome_data$descriptive_name

# subtract dark from not dark for all county / minute combinations
dark_predcitions <- post_predictions %>%
  select(starts_with('Yes'))

not_dark_predcitions <- post_predictions %>%
  select(starts_with('No'))

diff_dark_not_dark <- dark_predcitions - not_dark_predcitions

# calcualte hdi for all differences between dark and not dark for each county / time combination
colnames_for_hdi <- colnames(diff_dark_not_dark)

# create final dataset for plotting
discrimination_hdi_bart <- map_df(colnames_for_hdi, ~get_hdi(diff_dark_not_dark, .x)) %>%
  # separate county name and minute into two differenct columns
  separate(county_time, into = c('county', 'time'), sep = '_') %>%
  # convert minutes since midnight to time (hour / minute)
  mutate(time = as.numeric(time)) %>%
  mutate(hour_minute = minute_to_time(time)) %>%
  # add county populations so we can order facets by population
  left_join(county_pop, by = c('county' = 'NAME')) %>%
  mutate(county = fct_reorder(county, value, .desc = T))

```

```{r fig.width = 12, fig.height = 10, plotDifference, fig.cap = "BART model - There is not a difference between when it is dark and when it is not dark in the percentage of drives pulled over who are Black at a given time in the day."}

# plots of counties and difference in probability of being black between dark and not dark

discrimination_hdi_bart %>%
  ggplot(aes(hour_minute, estimate)) +
    geom_line() +
    geom_ribbon(aes(ymin = .lower, ymax = .upper), alpha = .2) +
    scale_x_time(labels = function(x) {hour(x)}) +
    scale_y_continuous(labels = percent_format(accuracy = 1)) +
    facet_wrap(~county, ncol = 5) +
  geom_hline(yintercept = 0, linetype = 2, alpha = .5) +
    labs(y = 'Difference between darkness and day in the percentage of drivers pulled over who are Black\n
              (Probability that the driver is Black when it is dark minus the probability driver is Black when it is not dark)',
         x = "Hour of day\n(24 hour times)")
```

## Bayesian logistic regression

```{r bayesHeirMod, cache = T, eval = F}
t_prior <- student_t(df = 7, location = 0, scale = 2)

# bayesian hierarchical model
heir_mod <- stan_glm(
  is_black ~ is_dark*county_name + splines::ns(minute, df = 4) + splines::ns(subject_age, df = 4) + subject_sex + year,
  data = model_df,
  family = binomial(link = "logit"),
  prior = t_prior, prior_intercept = t_prior,
  # algorithm = "fullrank",
  QR = TRUE,
  seed = 123,
  cores = 4
)

write_rds(heir_mod, 'hier_mod.rds')
```

```{r heirModPostPred, cache = T, eval = F}
# predictions from test set that was added to model
post_predictions <- posterior_epred(heir_mod, newdata = potential_outcome_data, draws = 200) %>%
  as.data.frame()

colnames(post_predictions) <- potential_outcome_data$descriptive_name

# subtract dark from not dark for all county / minute combinations
dark_predcitions <- post_predictions %>%
  select(starts_with('Yes'))

not_dark_predcitions <- post_predictions %>%
  select(starts_with('No'))

diff_dark_not_dark <- dark_predcitions - not_dark_predcitions

# calcualte hdi for all differences between dark and not dark for each county / time combination
colnames_for_hdi <- colnames(diff_dark_not_dark)

# create final dataset for plotting
discrimination_hdi_hier <- map_df(colnames_for_hdi, ~get_hdi(diff_dark_not_dark, .x)) %>%
  # separate county name and minute into two differenct columns
  separate(county_time, into = c('county', 'time'), sep = '_') %>%
  # convert minutes since midnight to time (hour / minute)
  mutate(time = as.numeric(time)) %>%
  mutate(hour_minute = minute_to_time(time)) %>%
  # add county populations so we can order facets by population
  left_join(county_pop, by = c('county' = 'NAME')) %>%
  mutate(county = fct_reorder(county, value, .desc = T))

write_rds(discrimination_hdi_hier, 'discrimination_hdi_hier.rds')
```

```{r fig.width = 12, fig.height = 10, plotDifference, fig.cap = "Logistic regression model - There is not a difference between when it is dark and when it is not dark in the percentage of drives pulled over who are Black at a given time in the day."}
discrimination_hdi_hier <- read_rds('discrimination_hdi_hier.rds')

discrimination_hdi_hier %>%
  # filter(!(county %in% !!counties_to_remove)) %>%
  ggplot(aes(hour_minute, estimate)) +
    geom_line() +
    geom_ribbon(aes(ymin = .lower, ymax = .upper), alpha = .2) +
    scale_x_time(labels = function(x) {hour(x)}) +
    scale_y_continuous(labels = percent_format(accuracy = 1)) +
    facet_wrap(~county, ncol = 5) +
  geom_hline(yintercept = 0, linetype = 2, alpha = .5) +
    labs(y = 'Difference between darkness and day in the percentage of drivers pulled over who are Black\n
              (Probability that the driver is Black when it is dark minus the probability driver is Black when it is not dark)',
         x = "Hour of day\n(24 hour times)")

```

## Limitations of the veil of darkness test

The veil of darkness test, like all tests, has its limits. The first is the test's scope. It's not trying to discern bias writ-large, but a narrow type of bias: bias when police officers make the quick decision to pull someone over. But, there are multiple entry points of bias with law enforcement. As noted earlier, there might be racial bias is the deployment of law enforcement assets. This bias could feed into disparities in traffic stops without being picked up by the veil of darkness test.

But, the scope is even smaller than this. It only examines police officer bias in a small roughly three hour period when it's daytime part of the year and nighttime part of the year.

Additionally, many streets contain artificial lighting. The veil of darkness test assumes that officers do not know the race of the driver they are pulling over at night. often, however, this isn't true.

Finally, there are factors that correlate with race which our model doesn't consider. Vehicle make, model, and year could correlate with race and are visible to the officer at night. If stops correlate with these three items and these items also correlate with race then we would not pick up bias.

That said, the test is a form of evidence pointing to bias or the lack thereof. .