---
title: Residual Analysis of Law School and Bar Passage
author: admin
date: '2020-12-08'
slug: residual-analysis-of-law-school-and-bar-passage
categories:
  - bayesian analysis
tags: []
subtitle: 'Which law schools perform better than predicted on the bar exam given their median UGPA and LSAT scores?'
summary: ''
authors: []
lastmod: '2020-12-08T19:43:42-05:00'
featured: no
image:
  caption: '<span>Photo by <a href="https://unsplash.com/@hudsoncrafted?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText">Debby Hudson</a> on <a href="https://unsplash.com/s/photos/studying?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText">Unsplash</a></span>'
  focal_point: ''
  preview_only: no
projects: []
links:
- icon: github
  icon_pack: fab
  name: Github
  url: https://github.com/shanejorr/shanes-site/blob/main/content/post/2020-12-08-residual-analysis-of-law-school-and-bar-passage/index.en.Rmd
---

```{r setup}
knitr::opts_chunk$set(echo = F,
                      message = F,
                      warning = F,
                      error = F)
```

```{r importLibraries}
library(knitr)
library(rstanarm)
library(tidybayes)
library(gridExtra)
library(kableExtra)
library(glue)
library(patchwork)
library(bayesplot)
library(tidyverse)

# stan options
# detect and use max number of cores to run models in parellel
options(mc.cores = parallel::detectCores())
```

```{r}
aggregate_rates <- function(df, group_var) {
  
  df %>%
    group_by_at(group_var) %>%
    summarize(takers = sum(takers),
              passers = sum(passers)) %>%
    mutate(pass_rate = passers / takers) %>%
    ungroup()
  
}

# create ggplot theme for this post
post_theme <- theme_minimal() +
  theme(legend.position = 'bottom',
        plot.title = element_text(size = 11),
        plot.subtitle = element_text(size = 10),
        axis.title = element_text(size = 10))

theme_set(post_theme)
```

```{r dataImport}

# Data import and cleaning -----------------------------

# all the ABA datasets are on GitHub
# we will save the directory to the repo, for less typing
data_dir <- 'https://raw.githubusercontent.com/shanejorr/ABA-509-disclosures/main/data/'

#import bar passage for all schools data set
bar <- read_csv(glue("{data_dir}First-Time_Bar_Passage_(Jurisdiction-Level).csv")) %>%
  select(-schoolid, -calendaryear) %>%
  # rename year column for easier future reference
  rename(year = firsttimebaryear) %>%
  # only keep 2014 and later
  # our admissions data starts at class entering in 2011, which is 2014 bar year,
  # so 2014 is the earliest bar exam year we can use
  filter(year >= 2014)

#import admissions data so we can extract a school's UPGA and LSAT
admiss <- read_csv(glue("{data_dir}Admissions.csv"))  %>%
  select(schoolname, calendaryear, uggpa75, uggpa50, uggpa25, lsat75, lsat50, lsat25) %>%
  # students in these admissions classes will take bar in three years
  # so, add three to year, so it matches the bar year
  mutate(year = calendaryear+3) %>%
  # scale gpa and lsat meterics by converting to mean of 0 and standard deviation of 1
  mutate(across(uggpa75:lsat25, scale, .names = "{col}_std")) %>%
  # create an indicator variable that is the sum of the standardized lsat and gpa variables
  mutate(indicator = rowSums(select(., contains('_std')))) %>%
  # standardize this indicator variable
  mutate(indicator = scale(indicator)) %>%
  select(-calendaryear)

#combine bar results and admissions datasets
bar <- left_join(bar, admiss, by=c('schoolname', 'year')) %>%
  # remove WI because all law grads from WI are listed as passing the bar
  filter(state != "WI",
        # extreme outlier
        state != "PR")

# aggregate school rates by year
school_year_rates <- aggregate_rates(bar, c('schoolname', 'year')) %>%
  left_join(admiss, by = c('schoolname', 'year'))

us_rate <- bar %>%
  mutate(overall = 'US Rate') %>%
  aggregate_rates('overall') %>%
  select(pass_rate) %>%
  .[[1]]

# create dataset for modeling
bar_mod <- bar %>%
  select(schoolname, takers, passers, schoolpasspct, state, year, contains("_std"), indicator) %>%
  drop_na() %>%
  # only keep school / state / year combinations with at least 10 bar takers
  filter(takers >= 10)
```

## Exploration of LSAT and Undergrad GPA as predictors

### Relationship between bar passage and both undergraduate GPA and LSAT scores

```{r fig.height = 7, fig.width = 6}
# use the same alpha and size for each plot
plot_alpha <- .5
plot_size <- .8

set2_colors <- c('#e8400c', '#0971b2')

uggpa_plot <- ggplot(school_year_rates, aes(uggpa50, pass_rate)) +
  geom_point(color = set2_colors[1], alpha = plot_alpha, size = plot_size) +
  facet_wrap(~year, ncol=3) +
  scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +
  labs(title = 'Undergraduate GPA',
       x = 'Median school undergraduate GPA',
       y = 'School bar passage rate')

lsat_plot <- ggplot(school_year_rates, aes(lsat50, pass_rate)) +
  geom_point(color = set2_colors[2], alpha = plot_alpha, size = plot_size) +
  facet_wrap(~year, ncol=3) +
  scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +
  labs(title = 'LSAT',
       x = 'Median school LSAT score',
       y = 'School bar passage rate')

uggpa_plot / lsat_plot
```

## Correlation between median LSAT and median Undergrad GPA

```{r fig.height = 4, fig.width = 6, fig.caption = "Correlation between each school's standardized median LSAT and standardized median undergrad GPA. Blue line is the fit line of the data, red line is a hypothetical perfect fit line - a slope of 1. LSAT and GPA are highly correlated, with the actual fit line mirroring the perfect fit line."}
ggplot(school_year_rates, aes(scale(uggpa50), scale(lsat50))) +   
  geom_abline(intercept = 0, slope = 1, linetype = 2, size = 1,
              color = 'indianred2') +
  geom_smooth(method = 'lm', alpha = .7, se = F) +
  geom_point(alpha = .3, size = plot_size) +
  facet_wrap(~year, ncol=3) +
  labs(title = "Correlation Between a School's Median LSAT and Median Undergrad GPA",
       subtitle = "Values are standardized",
       x = 'Standardized median school undergraduate GPA',
       y = 'Standardized median school LSAT')
```

## Difference between predicted and actual bar passage

### Models used

```{r pca}
bar_pca <- bar_mod %>%
  select(contains("_std")) %>%
  prcomp() 

bar_pca_summary <- bar_pca %>%
  summary(.) %>%
  .[6] %>%
  as.data.frame()

# get first two principal component values
bar_pca_x <- bar_pca$x %>%
  as.data.frame() %>%
  select(PC1, PC2)

# add princ component values to bar dataset
bar_mod <- bar_mod %>%
  bind_cols(bar_pca_x)

# create column names for PCA table
pca_cols <- glue("PC {seq(1, 6)}")

var_explained <- round(bar_pca_summary[[2]][3] * 100, 0)

bar_pca_summary %>%
  kable(col.names = pca_cols)
```

```{r barMods, cache = T, eval = T}
school_mod_linear <- stan_glmer(cbind(passers, takers - passers) ~ lsat50_std + uggpa50_std + 
                                                                    (1 | state) + (1 | year),
                              prior_intercept = student_t(1.25, 1, 7),
                              iter = 2000,
                              seed = 145,
                              data = bar_mod,
                              family = binomial("logit"))

write_rds(school_mod_linear, 'school_mod_linear.rds')

school_mod_poly <- stan_glmer(cbind(passers, takers - passers) ~ poly(lsat50_std, 2) + poly(uggpa50_std, 2) + 
                                                                  (1 | state) + (1 | year),
                              prior_intercept = student_t(1.25, 1, 7),
                              iter = 2000,
                              seed = 145,
                              data = bar_mod,
                              family = binomial("logit"))

write_rds(school_mod_poly, 'school_mod_poly.rds')

school_mod_pca <- stan_glmer(cbind(passers, takers - passers) ~ PC1 + PC2 + (1 | state) + (1 | year),
                              prior_intercept = student_t(1.25, 1, 7),
                              iter = 2000,
                              seed = 145,
                              data = bar_mod,
                              family = binomial("logit"))

write_rds(school_mod_pca, 'school_mod_pca.rds')

school_mod_indicator <- stan_glmer(cbind(passers, takers - passers) ~ indicator +
                                                                (1 | state) + (1 | year),
                              prior_intercept = student_t(1.25, 1, 7),
                              iter = 2000,
                              seed = 145,
                              data = bar_mod,
                              family = binomial("logit"))

write_rds(school_mod_indicator, 'school_mod_indicator.rds')

school_mod_spline <- stan_glmer(cbind(passers, takers - passers) ~ splines::ns(lsat50_std, df=2) + 
                                                                    splines::ns(uggpa50_std, df=2) + 
                                                                    (1 | state) + (1 | year),
                              prior_intercept = student_t(1.25, 1, 7),
                              iter = 2000,
                              seed = 145,
                              data = bar_mod,
                              family = binomial("logit"))

write_rds(school_mod_spline, 'school_mod_spline.rds')

mod_list <- list(school_mod_linear, school_mod_poly, school_mod_pca, school_mod_indicator, school_mod_spline)

mod_loo_list <- map(mod_list, loo)

mod_loo_list <- list(linear = mod_loo[[1]],
                     poly = mod_loo[[2]],
                     pca = mod_loo[[3]],
                     indicator = mod_loo[[4]],
                     spline = mod_loo[[5]])

write_rds(mod_loo_list, "school_mod_loo.rds")

write_rds(mod_list, 'mod_list.rds')
```


### Compare models

#### Difference in LOOs

```{r looCompare}

loo_compare(mod_loo_list)
```

#### Model weights

```{r}
loo_model_weights(mod_loo_list)
```

#### Poserior predictive checks

```{r postpredictions, cache = T}

# function to create posterior predictive check plot
ppc_plot <- function(mod, num_takers, pass_pct, plot_title) {
  
  # creates posterior predictions and then creates ppc plot
  
  pred_draws <- posterior_predict(mod, draws = 50) %>%
                t()
  
  # calculate percentage probability from number of passers in prediction
  pred_draws <- t(pred_draws / num_takers)
  
  # create plot
  ppc_dens_overlay(yrep = pred_draws, y = pass_pct) +
    ggtitle(plot_title)
  
}

# titles for plots, in same order as models
plot_titles <- c('Linear', 'Poly', 'PCA', 'Indicator', 'Spline')

ppc_mod_plots <- map2(mod_list, plot_titles, 
                      ppc_plot, num_takers = bar_mod$takers, pass_pct = bar_mod$schoolpasspct)

```

```{r ppcPlots, fig.height = 6, fig.width = 8}
wrap_plots(ppc_mod_plots, nrow = 2)
```

```{r}
# create fitted draws and HPI of fitted draws

predicted_pass_draws <- bar_mod %>%
  # create posterior predictions on dataset
  add_fitted_draws(school_mod_pca, n = 200, seed = 938) %>%
  left_join(aggregate_rates(bar, c('schoolname', 'year', 'state')), by = c('schoolname', 'year', 'state')) %>%
  select(-ends_with(".y")) %>%
  # calculate residuals (actual pass rate minus predicted value)
  mutate(residual = pass_rate - .value)

predicted_pass_hdi <- predicted_pass_draws %>%
  # we want summary statistics by school
  group_by(schoolname) %>%
  median_hdci(.value) %>%
  select(schoolname, .value, .lower, .upper) %>%
  # add overall school pass rates
  left_join(aggregate_rates(bar, 'schoolname'), by = 'schoolname') %>%
  ungroup()
```


```{r}
# calcualte HDI of residuals
predicted_pass_residual <- predicted_pass_draws %>%
  ungroup() %>%
  # we want summary statistics by school
  group_by(schoolname) %>%
  median_hdci(residual) %>%
  select(schoolname, residual, .lower, .upper)
```

```{r}
# create plot of actual and expected values

# plot actual pass rates and predictions
n_rows <- nrow(predicted_pass_hdi)

# point size for all points in all plots
plot_point_size <- .7

plot_actual <- predicted_pass_hdi %>%
  ggplot(aes(.value, fct_reorder(schoolname,pass_rate))) +
  # bayesian predicted pass rate
  geom_point(alpha=.6, color = set2_colors[1], size = plot_point_size) +
  geom_vline(xintercept = us_rate, linetype = 2, alpha = .7) +
  annotate('text', x = us_rate - .02, y = n_rows-4, 
           angle = 90,  alpha = .7, size = 3,
           label = "Overall pass rate") +
  # bayesian 90% HPI
  geom_errorbarh(aes(xmax = .lower, xmin = .upper), color = set2_colors[1]) +
  # actual bar passage rate
  geom_point(aes(pass_rate, fct_reorder(schoolname, desc(.value))), 
                 color = set2_colors[2], size = plot_point_size) +
  scale_x_continuous(labels = scales::percent_format(accuracy=1)) +
  labs(x="Predicted and actual\nBar passage rates",
       y=NULL)
```

```{r fig.height = 12}
# calculate probability that predicted value is lower than actual value, to be plotted
prob_greater_zero <- predicted_pass_draws %>%
  mutate(greater_zero = residual > 0) %>%
  group_by(schoolname) %>%
  summarize(perc_greater_zero = sum(greater_zero) / n()) %>%
  mutate(perc_greater_zero = glue("{round(perc_greater_zero*100, 0)}%")) %>%
  select(schoolname, perc_greater_zero)

# create plot of residuals
plot_residuals <- predicted_pass_residual %>%
  # add pass rate, so we can order by it in the plot
  left_join(predicted_pass_hdi[c('schoolname', 'pass_rate')], by = 'schoolname') %>%
  left_join(prob_greater_zero, by = 'schoolname') %>%
  mutate_at(vars(residual, .lower, .upper), ~.*100) %>%
  ggplot(aes(residual, fct_reorder(schoolname,pass_rate))) +
  # bayesian predicted pass rate
  geom_point(alpha=.6, color = set2_colors[1], size = plot_point_size) +
  geom_vline(xintercept = 0, linetype = 2, alpha = .7) +
  # bayesian 95% HPI
  geom_errorbarh(aes(xmax = .lower, xmin = .upper), color = set2_colors[1]) +
  geom_text(aes(x = 40, y = schoolname, label = perc_greater_zero)) +
  labs(x='Actual minus predicted\nBar passage rate',
       y=NULL) +
  theme(axis.text.y=element_blank())

```

```{r, fig.height = 25, fig.width = 10, fig.caption = "Left plot reveals actual and predicted bar passage rates. Right plot shows residuals: actual minus predicted bar passage rates. The percentage on the right is the probability that the actual bar passage rate is higher than the predicted rate. For almost all schools, the 95% credible intervals cross zero. This highlights that there is little we can learn from the residuals about school-level performance on the bar exam."}
# combine two plots and display results

plot_actual + plot_residuals +
  plot_annotation(
  title = 'Actual and Predicted Bar Passage Rates (left) and Residuals (right)',
  subtitle = 'Bands are 95% credible intervals | Percentage is probability that actual rate is higher than predicted rate'
)
```