---
title: Using Bayesian Additive Regression Trees to Analyze Traffic Stop Bias
author: ''
date: '2021-03-08'
slug: using-bayesian-additive-regression-trees-to-analyze-traffic-stop-discrimination
categories:
  - bayesian analysis
  - traffic stops
  - Bayesian Additive Regression Trees
tags: []
subtitle: ''
summary: ''
authors: []
lastmod: '2021-03-08T19:22:36-05:00'
featured: no
image:
  caption: '<span>Photo by <a href="https://unsplash.com/@gerandeklerk?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText">Geran de Klerk</a> on <a href="https://unsplash.com/s/photos/forest?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText">Unsplash</a></span>'
  focal_point: ''
  preview_only: no
projects: []
links:
- icon: github
  icon_pack: fab
  name: Github
  url: https://github.com/shanejorr/shanes-site/blob/main/content/post/2021-03-08-using-bayesian-additive-regression-trees-to-analyze-traffic-stop-discrimination/index.en.Rmd
draft: yes
---

<script src="{{< relref "post/2021-03-08-using-bayesian-additive-regression-trees-to-analyze-traffic-stop-discrimination/index.en.html" >}}index.en_files/header-attrs/header-attrs.js"></script>


<p>Bayesian Additive Regression Trees (BART) are a Bayesian version of random forests. They’re flexible, non-parametric, and supply a full posterior distribution on outcomes. They have all the benefits of Bayesian modeling and non-parametric machine learning modeling in one algorithm.</p>
<p>In this post, I practice with BART models using traffic stop data as the backdrop. We’ll apply BART to the veil of darkness test and see if we can uncover racial disparities on traffic stops.</p>
<div id="why-bart" class="section level2">
<h2>Why BART?</h2>
<p>We’re not going into the technical details of BART here. Instead, check out the following resources To understand the algorithm’s guts:</p>
<ul>
<li><a href="https://arxiv.org/abs/0806.3286">BART: Bayesian additive regression trees</a> - The paper that introduced the world to BART.<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a></li>
<li><a href="https://www.tandfonline.com/doi/abs/10.1198/jcgs.2010.08162">Bayesian Nonparametric Modeling for Causal Inference</a> - Jennifer Hill’s paper that brought us BART for causal inference.<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a></li>
<li><a href="https://www.youtube.com/watch?v=mJVTg8PWjW4">Causal Inference at the Intersection of Statistics and Machine Learning</a> - Video presentation by Jennifer Hill on BART fo causal inference. The video isn’t great quality, but it’s an easy to follow introduction to BART.</li>
<li><a href="https://www.youtube.com/watch?v=9d5-3_7u5a4">Introduction to Bayesian Additive Regression Trees (BART) for Causal Inference</a> - Another good video introduction to BART for causal inference.</li>
</ul>
<p>In general terms, however, I view the pros of BART as falling into two categories: (1) reasons it’s better than parametric Bayesian models such as linear or logistic regression, and (2) reasons it’s better than other non-parametric methods like random forests. Let’s look at these two categories.</p>
<div id="why-bart-is-often-better-than-parametric-bayesian-methods" class="section level3">
<h3>Why BART is (often) better than parametric Bayesian methods</h3>
<p>Compared to parametric models, BART lets you easily and automatically discover a prediction model’s best functional form. The best, at least, in terms of partitioning the feature space as trees. In a way, it’s plug and play.</p>
<p>There are a few reasons why this brings benefits over parametric Bayesian models. First, no more worrying about the assumption of linearity between the predictors and the outcome. No more fishing around with splines or polynomials just in case the relationship is nonlinear. If it’s nonlinear, the algorithm will create a nonlinear fit without you intervening.</p>
<p>For causal inference, the algorithm automatically incorporates interactions to the degree they appear in the data. No more trying multiple models, some with and some without interactions, to see if there are interaction effects. You get them for free.</p>
<p>Finally, variable selection is simplified. Predictors that do not predict the outcome will simply not find their way into any trees. Humans don’t have to identify and exclude them, the model identifies and excludes them.</p>
</div>
<div id="why-bart-is-often-better-than-other-non-parametric-methods" class="section level3">
<h3>Why BART is (often) better than other non-parametric methods</h3>
<p>There’s one answer here: you get the full posterior distribution of your outcome just like with any Bayesian model. This is crucial with causal inference as it let’s you map the uncertainty of your treatment effects. Black-box machine learning models don’t do this. They give you one answer and tell you little about uncertainty in the answer.</p>
<pre class="r"><code>library(glue)
library(vroom)
library(modelr)
library(tidycensus)
library(lubridate)
library(highcharter)
library(scales)
library(widgetframe) 
library(suncalc)
library(tigris)
library(sf)
library(bartCause)
library(tidybayes)
library(lme4)
library(gt)
library(tidyverse)</code></pre>
<pre class="r"><code># custom functions --------------------------

# create ggplot theme for this post
post_theme &lt;- theme_minimal() +
  theme(legend.position = &#39;bottom&#39;,
        plot.title = element_text(size = 11),
        plot.subtitle = element_text(size = 10), 
        axis.title = element_text(size = 10))

theme_set(post_theme)

# standard credits to all to all high charts plots

# credits to be used on all plots
add_credits &lt;- function(hc) {
  hc %&gt;%
    hc_credits(
      enabled = TRUE,
      text = &quot;Data source: The Stanford Open Policing Project | US Census Bureau Pop. Estimates&quot;,
      href = FALSE
  )
}

scatter_stops_per &lt;- function(df, second_race, plot_title, y_title) {
  
  # scatter plot of stops per 100 residents for white and black / hispanic drivers
  # second_race is the race to compare with white
  
  # max axis point
  max_point &lt;- 45
  
  # find out if white or minority group is higher, used for colors above or below line
  df[[&#39;diff&#39;]] &lt;- ifelse(df[[&#39;white&#39;]] &lt; df[[second_race]], 1, 0)
  
  # assign color to each row
  color_palette &lt;- c(&#39;#4e79a7&#39;, &#39;#f28e2b&#39;)
  df[[&#39;colors&#39;]] &lt;- colorize(df[[&#39;diff&#39;]], color_palette)
  
  # tooltip
  scatter_tooltip &lt;- &quot;&lt;b&gt;{point.county_name}&lt;/b&gt;&lt;br&gt;
                      Stops per 100 residents (Black): &lt;b&gt;{point.black}&lt;/b&gt;&lt;br&gt;
                      Stops per 100 residents (White): &lt;b&gt;{point.white}&lt;/b&gt;&lt;br&gt;
                      Stops per 100 residents (Hispanic): &lt;b&gt;{point.hispanic}&lt;/b&gt;&lt;br&gt;
                      County population (2013): &lt;b&gt;{point.value_tooltip}&lt;/b&gt;&lt;br&gt;&quot;
  
  df %&gt;%
    hchart(
      &quot;bubble&quot;, 
      hcaes(x = &#39;white&#39;, y = .data[[second_race]], size = value, color = colors)
    ) %&gt;%
    hc_add_series(name = &#39;line&#39;,
                  data = data.frame(
                    x = seq(1, max_point, 1),
                    y = seq(1, max_point, 1),
                    type = &#39;line&#39;,
                    marker = list(enabled = FALSE)
                  )) %&gt;%
    # remove tooltip for line
    hc_plotOptions(line = list(enableMouseTracking = FALSE),
                   bubble = list(opacity = .85,
                                 minSize = &quot;1%&quot;, maxSize = &quot;7%&quot;)) %&gt;%
    hc_xAxis(min = 1, max = max_point, type = &#39;logarithmic&#39;,
             title = list(text = &#39;Stops per 100 residents for White drivers&#39;)) %&gt;% 
    hc_yAxis(min = 1, max = max_point, type = &#39;logarithmic&#39;,
             title = list(text = y_title)) %&gt;%
    hc_tooltip(
        headerFormat = &quot;&quot;,
        pointFormat = scatter_tooltip
    ) %&gt;%
    hc_title(text = plot_title)
}

# extract time from date time
time_to_minute &lt;- function(time) {
  # minutes since midnight
  hour(time) * 60 + minute(time)
}

create_train_test_sets &lt;- function(split_partition, predictor_variables, train_or_test = &#39;train&#39;) {
  
  # create train and testing datasets
  
  if (train_or_test == &#39;train&#39;) {
    
    pred_df &lt;- training(split_partition)
    
  } else if (train_or_test == &#39;test&#39;) {
    
    pred_df &lt;- testing(split_partition)
    
  } else {
    stop(&quot;train_or_test must be either &#39;train&#39; or &#39;test&#39;&quot;)
  }
  
  pred_df %&gt;%
      ungroup() %&gt;%
      # only keep predictors (required for BART)
      select(!!predictor_variables) %&gt;%
      as.data.frame()
  
}</code></pre>
<pre class="r"><code># downlaod traffic stop data --------------------------

# download entire state traffic stop dataset
nc_stops &lt;- vroom(&#39;https://shane-datasets.nyc3.digitaloceanspaces.com/traffic-stop/nc/nc_statewide_2020_04_01.csv.gz&#39;,
                  col_select = -contains(&#39;raw&#39;))

# get 2010 - 2013 5 year and aggregate
pop_years &lt;- seq(2009, 2013, 1)

n_years &lt;- length(pop_years)

nc_stops &lt;- nc_stops %&gt;%
  mutate(year = year(date)) %&gt;%
  # only keep between years 2010 and 2013 
  # 2013 is final full year of stop data
  filter(year %in% !!pop_years)</code></pre>
</div>
</div>
<div id="comparing-stops-per-100-residents-among-racial-groups" class="section level2">
<h2>Comparing Stops Per 100 Residents Among Racial Groups</h2>
<p>Prior to modeling racial bias with the veil of darkness test, let’s take a look at the data. The traffic stop data set comes from <a href="https://openpolicing.stanford.edu/">The Stanford Open Policing Project</a> and is the same data used in <a href="https://www.shaneorr.io/post/exploratory-analysis-of-north-carolina-traffic-stop-data/">the last post</a>.</p>
<pre class="r"><code># racial populations by county from census ----------------

# recode integers for dates to years
recode_date &lt;- c(
  `3` = 2010,
  `4` = 2011,
  `5` = 2012,
  `6` = 2013,
  `7` = 2014,
  `8` = 2015
)

# get 2013 overall county populations
county_pop &lt;- get_estimates(geography = &quot;county&quot;, state = &#39;NC&#39;,
                            product = &#39;population&#39;, 
                            time_series = T,
                            year = 2018) %&gt;%
  # only keep 2013 and only keep population (not density)
  filter(DATE == 6,
         variable == &#39;POP&#39;) %&gt;%
  mutate(NAME = str_remove_all(NAME, &#39; County, North Carolina&#39;)) %&gt;%
  select(NAME, value)

# import population data by county and race
county_pop_race &lt;- get_estimates(geography = &quot;county&quot;, state = &#39;NC&#39;,
                                product = &#39;characteristics&#39;, 
                                breakdown = c(&#39;RACE&#39;, &#39;HISP&#39;),
                                breakdown_labels = T,
                                time_series = T,
                                year = 2018) 

# calculate aggregate percentage of population by race
county_pop_agg &lt;- county_pop_race %&gt;%
  # only keep 2010 - 2013
  filter(between(DATE, 3, 6)) %&gt;%
  # aggregate population by race for each county
  group_by(GEOID, NAME, RACE, HISP) %&gt;%
  summarize(agg_pop = sum(value)) %&gt;%
  ungroup() %&gt;%
  filter(HISP != &#39;Both Hispanic Origins&#39;) %&gt;%
  # make the All Races  race that is Hisp the Hisp race
  mutate(RACE = ifelse(RACE == &#39;All races&#39; &amp; HISP == &#39;Hispanic&#39;, &#39;Hispanic&#39;, RACE)) %&gt;%
  # only need white, black, and hispanic
  filter(RACE %in% c(&#39;White alone&#39;, &#39;Black alone&#39;, &#39;Hispanic&#39;),
         # do not need hispanic breakdown by race
         RACE == &#39;Hispanic&#39; | HISP == &#39;Non-Hispanic&#39;) %&gt;%
  # remove &#39;alone&#39; phrase from race
  mutate(RACE = str_remove_all(RACE, &#39; alone&#39;),
         # make lower case and remove latino to match traffic stops
         RACE = str_to_lower(RACE),
         # remove string so that it matches with traffic stop data
         NAME = str_remove_all(NAME, &#39; County, North Carolina&#39;)) %&gt;%
  # don&#39;t need the hispanic column anymore
  select(-HISP)</code></pre>
<pre class="r"><code># calculate stops per race --------------

# save list of stop reasons to be uses in text
stop_reasons &lt;- unique(nc_stops$reason_for_stop)

stops_by_race &lt;- nc_stops %&gt;%
  # calculate number of stops by race (numerator)
  group_by(county_name, subject_race) %&gt;%
  summarize(num_stops_race = n()) %&gt;%
  # calculate percentage of stops by race
  ungroup() %&gt;%
  # stops per 100 people
  #mutate(stops_per = (num_stops_race*100) / agg_pop) %&gt;%
  # only need three races; not enough data for others
  filter(subject_race %in% c(&#39;black&#39;, &#39;white&#39;, &#39;hispanic&#39;)) %&gt;%
  # only keep counties with a minimum of 100 stops for each race
  group_by(county_name) %&gt;%
  mutate(min_num = min(num_stops_race)) %&gt;%
  ungroup() %&gt;%
  filter(min_num &gt;= 500) %&gt;%
  drop_na(county_name) %&gt;%
  mutate(county_name = str_remove_all(county_name, &#39; County&#39;),
         # correct spelling mistake
         county_name = str_replace(county_name, &#39;Tyrell&#39;, &#39;Tyrrell&#39;)) %&gt;%
  drop_na(county_name) %&gt;%
  # combine stops by race percentages and racial population percentages
  left_join(county_pop_agg, 
            by = c(&#39;county_name&#39; = &#39;NAME&#39;, &#39;subject_race&#39; = &#39;RACE&#39;)) %&gt;%
  # number of stops per 100 people
  mutate(stops_per = round(num_stops_race / (agg_pop / 100), 2)) %&gt;%
  # convert to wide form where each race is in a different column
  # needed for plotting
  pivot_wider(id_cols = &#39;county_name&#39;, 
              names_from = &#39;subject_race&#39;,
              values_from = &quot;stops_per&quot;) %&gt;%
  # combine county populations
  left_join(county_pop,
            by = c(&#39;county_name&#39; = &#39;NAME&#39;)) %&gt;%
  mutate(value_tooltip = number(value, accuracy = 1, big.mark = &#39;,&#39;)) %&gt;%
  # Alleghany is odd outlier, so remove
  filter(!county_name %in% c(&#39;Alleghany&#39;))</code></pre>
<p>One way to show racial differences in traffic stops is to compare stops per 100 residents between racial groups. The graph below shows this comparison for all North Carolina counties. The left plot compares Black and White drivers, while the right plot highlights Hispanic and White drivers. Each point is a county. The diagonal line is the trend we would expect if drivers of different races have the same number of stops per 100 residents.</p>
<p>For almost all counties in North Carolina, Black drivers are stopped at a higher rate per 100 residents than White drivers. This is shown by almost all of the points lying above the diagonal line for the Black / White comparison chart. The Hispanic and White rates, however, are similar as shown by the dots in the Hispanic / White graph falling along the diagonal line.</p>
<pre class="r"><code># scatter plot of race and perc. stops compared to overall perc. pop ----------


bw_stops_title &lt;- &#39;Stops Per 100 Residents for Black and White Drivers&#39;
bw_y_title &lt;- &#39;Stops per 100 residents for Black drivers&#39;
bw_scatter_plot &lt;- scatter_stops_per(stops_by_race, &#39;black&#39;, bw_stops_title, bw_y_title)

hw_stops_title &lt;- &#39;Stops Per 100 Residents for Hispanic and White Drivers&#39;
hw_y_title &lt;- &#39;Stops per 100 residents for Hispanic drivers&#39;
hw_scatter_plot &lt;- scatter_stops_per(stops_by_race, &#39;hispanic&#39;, hw_stops_title, hw_y_title) %&gt;%
  add_credits()

hw_grid(
  bw_scatter_plot, hw_scatter_plot,
  ncol = 2, rowheight = 300
)</code></pre>
<p>Of course, stopping our analysis here is unwarranted. First, this does not account for how often different racial groups drive. It’s comparing stops per 100 residents, not stops per 100 miles driven. Second, we have not accounted for differences in policing patterns. In general, police patrol Black neighborhoods more frequently than White neighborhoods. Therefore, the greater police presence could account for the differences in stops per 100 residents. Although, it’s no vindication of police to explain away the differences in stops per 100 residents by saying that their neighborhoods are more heavily patrolled. This simply kicks the bias up a step to the level of the deployment of policing assets. Finally, drivers within different racial groups may just drive differently.</p>
</div>
<div id="veil-of-darkenss-test" class="section level2">
<h2>Veil of darkenss test</h2>
<p>The veil of darkness test attempts to overcome these differences. The test has its own weaknesses and should only be seen as additional evidence. It can provide evidence for or against bias in police stops, but should never be taken as the final say.</p>
<div id="what-is-the-veil-of-darkness-test" class="section level3">
<h3>What is the veil of darkness test?</h3>
<p>Here’s how <a href="https://www.nature.com/articles/s41562-020-0858-1">a group of scholars</a> explained the test:</p>
<blockquote>
<p>[The] method starts from the idea that officers who engage in racial profiling are less able to identify a driver’s race after dark than during the day. As a result, if officers are discriminating against black drivers—all else being equal—one would expect black drivers to comprise a smaller share of stopped drivers at night, when a veil-of-darkness masks their race. To account for patterns of driving and police deployment that may vary throughout the day, the test leverages the fact that the sun sets at different times during the year.<a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a></p>
</blockquote>
<p>For example, let’s say it is dark at 7 p.m. for a third of the year, light for a third of the year, and kind-of dark for a third of the year. We want to compare the percentage of all drivers pulled over are Black and when it’s light at 7 p.m. against the percentage of all drivers are pulled over who are Black when it’s dark at 7 p.m. If a larger percentage are pulled over when it’s light we have some evidence of racial bias.</p>
<p>This reduces to a prediction problem. For each traffic stop, we want to predict the driver’s race given the time of the stop, the location (county), whether it’s dark out, and other possible covariates. Then, we’ll turn around and predict every driver’s race. If the probability that a driver is Black is higher during the day than at night - holding time, location, and other covariates constant - then we have evidence of racial bias.</p>
<p>Researchers generally apply logistic regression to this problem. And that’s a great tool. But, we’re here to learn BART, so that what we’re applying. Plus, BART provides all the benefits over parametric models that we mentioned in the beginning.</p>
<pre class="r"><code># create dataset for veil of darkness test -----------------------

# get centroid of every county

# get county shapefiles and centroids
nc_counties &lt;- counties(state = &#39;NC&#39;, cb = T)

# find centroid of each county by using it&#39;s boundaries
nc_counties$lon&lt;-st_coordinates(st_centroid(nc_counties))[,1]
nc_counties$lat&lt;-st_coordinates(st_centroid(nc_counties))[,2]

# drop shapefile column
nc_counties$geometry &lt;- NULL

nc_counties &lt;- nc_counties %&gt;%
  ungroup() %&gt;%
  select(GEOID, NAME, lon, lat)

# get sunlight times for each county

# extend dataset to include a county row for each day
# neede because function to calcualte sunset requires date column in dataframe
days_seq &lt;- seq(as.Date(min(nc_stops$date)), as.Date(max(nc_stops$date)), &quot;days&quot;)

nc_counties_sunset &lt;- map_df(days_seq, function(day) {
  nc_counties %&gt;%
    mutate(date = !!day) %&gt;%
    getSunlightTimes(
      data = .,
      keep = c(&quot;sunset&quot;, &quot;dusk&quot;), 
      tz = &#39;EST&#39;
    )
}) %&gt;%
  mutate(
    sunset_minute = time_to_minute(sunset),
    dusk_minute = time_to_minute(dusk),
    date = ymd(str_sub(date, 1, 10))
) %&gt;% 
  # join county names
  left_join(nc_counties, by = c(&#39;lon&#39;, &#39;lat&#39;))

# only keep the 20 most populated counties
most_pop_counties &lt;- county_pop %&gt;%
  arrange(desc(value)) %&gt;%
  head(20) %&gt;%
  .[[1]]

# merge dusk times with nc stops dataset
nc_stops_veil &lt;- nc_stops %&gt;%
  drop_na(time) %&gt;%
  mutate(county_name = str_remove_all(county_name, &#39; County&#39;)) %&gt;%
  # only keep the 20 most populated counties
  filter(county_name %in% !!most_pop_counties) %&gt;%
  # add sunset and dusk times
  left_join(nc_counties_sunset, 
            by = c(&#39;county_name&#39; = &#39;NAME&#39;, &#39;date&#39;)) %&gt;%
  # convert date times to integer minutes from midnight
  mutate(
    minute = time_to_minute(time),
    minutes_after_dark = minute - dusk_minute,
    is_dark = minute &gt; dusk_minute
  ) %&gt;%
  ungroup() %&gt;%
  group_by(county_name) %&gt;%
  # find the min and max dusk times for each county
  mutate(
    min_dusk_minute = min(dusk_minute),
    max_dusk_minute = max(dusk_minute),
    is_black = subject_race == &quot;black&quot;
  ) %&gt;% 
  filter(
    # Filter to get only the intertwilight period
    minute &gt;= min_dusk_minute,
    minute &lt;= max_dusk_minute,
    # Remove ambigous period between sunset and dusk
    !(minute &gt; sunset_minute &amp; minute &lt; dusk_minute),
    # Compare only white and black drivers
    subject_race %in% c(&quot;black&quot;, &quot;white&quot;)
  ) %&gt;%
  select(date, time, sunset, dusk, contains(&#39;minute&#39;), is_dark, everything()) %&gt;%
  drop_na(is_black, is_dark, minute, subject_age, subject_sex, county_name) %&gt;%
  mutate(county_name = factor(county_name),
         year = as.factor(year))

# remove items to save RAM
rm(county_pop_race)
rm(nc_stops)
rm(nc_counties_sunset)
gc()</code></pre>
<div id="section" class="section level45">
<p class="heading"></p>
<pre class="r"><code># create test and train datasets -------------------
# only keep the variables we need to make the train and test sets not take up as much RAM
vars &lt;- c(&#39;minute&#39;, &#39;subject_age&#39;, &#39;subject_sex&#39;, &#39;county_name&#39;, &#39;is_dark&#39;, &#39;year&#39;, &#39;is_black&#39;)
partition_group_vars &lt;- seq(4, 7, 1)
# BART requires categoricals with more than one level to be factors
model_df &lt;- nc_stops_veil %&gt;% 
  ungroup() %&gt;%
  select(!!vars) %&gt;%
  drop_na() %&gt;%
  mutate(across(c(&#39;subject_sex&#39;, &#39;county_name&#39;, &#39;year&#39;), ~as.factor(.)))
response_train &lt;- as.numeric(model_df$is_black)
predictor_df &lt;- model_df %&gt;% 
  select(-is_black) %&gt;%
  as.data.frame()
# dataset to create potential outcomes
# contains each column of the original dataset twice - one with dark, one with not-dark
ate_data &lt;- model_df %&gt;%
  mutate(is_dark = T) %&gt;%
  bind_rows(
    model_df %&gt;%
      mutate(is_dark = F)
  ) %&gt;%
  select(-is_black) %&gt;%
  as.data.frame()</code></pre>
<pre class="r"><code># run BART model --------------------------------
bart_mod &lt;- mc.pbart(
  x.train = predictor_df, 
  y.train = response_train, 
  x.test = ate_data, 
  keeptrainfits = F,
  ndpost = 200, 
  mc.cores = 4,
  cont = FALSE,
  seed = 99
)
write_rds(bart_mod, &#39;bart_mod.rds&#39;)</code></pre>
<pre class="r"><code>bart_mod &lt;- read_rds(&#39;bart_mod.rds&#39;)
post_pred_ate &lt;- as.data.frame(bart_mod$prob.test)
# find most likely difference in probability driver is black when comparing before and after sunset
n &lt;- ncol(post_pred_ate)
dark_end &lt;- n/2
light_start &lt;- dark_end + 1
post_pred_dark &lt;- post_pred_ate[1:dark_end][1,2]
post_pred_not_dark &lt;- post_pred_ate[light_start:n][1,2]
post_pred_outcomes &lt;- post_pred_dark - post_pred_not_dark</code></pre>
<pre><code>
```r
# create predictions to calculate average treatment effect ------------------------------------
# dataset to create potential outcomes
# contains each column of the original dataset twice - one with dark, one with not-dark
pred_data &lt;- model_df %&gt;%
  mutate(is_dark = T) %&gt;%
  bind_rows(
    model_df %&gt;%
      mutate(is_dark = F)
  ) %&gt;%
  select(-is_black)
pred_ate &lt;- predict(bart_mod, pred_data, mc.cores = 6)
# create training / test partitions
nc_stop_splits &lt;- model_df %&gt;%
  group_by_at(vars[partition_group_vars]) %&gt;%
  initial_split(prop = .75)
# extracgt train / test datasets
nc_stops_train &lt;- create_train_test_sets(nc_stop_splits, vars, &#39;train&#39;)
response_train &lt;- as.numeric(nc_stops_train$is_black)
nc_stops_train &lt;- select(nc_stops_train, -is_black)
nc_stops_test &lt;- create_train_test_sets(nc_stop_splits, vars, &#39;test&#39;) %&gt;%
  select(-is_black)
# remove to clean up RAM
rm(nc_stop_splits)</code></pre>
<pre class="r"><code>a &lt;- data.frame(
  a = round(rnorm(10, 0, 5), 0),
  b = round(rnorm(10, 0, 5), 0)
)
b &lt;- data.frame(
  c = round(rnorm(10, 0, 5), 0),
  d = round(rnorm(10, 0, 5), 0)
)
a - b
# find most likely difference in probability driver is black when comparing before and after sunset
n_test_start &lt;- n+1
n_test_end &lt;- nrow(test_data)
post_pred_dark &lt;- posterior_predictions[1:48661][1,2]
post_pred_not_dark &lt;- posterior_predictions[48662:97322][1,2]
post_pred_outcomes &lt;- post_pred_dark - post_pred_not_dark</code></pre>
<pre class="r"><code># find the difference between 
avg_prob_is_black &lt;- function(pred_matrix, vec_with_dark, use_dark_cols) {
  
  pred_df &lt;- as.data.frame(pred_matrix)
  
  pred_df &lt;- pred_df[vec_with_dark == use_dark_cols]
  
  pred_df %&gt;%
    summarize_all(mean) %&gt;%
    as_vector()
}
# mean difference in dark
post_dark &lt;- avg_prob_is_black(county_predictions, nc_stops_veil$is_dark, T)
post_light &lt;- avg_prob_is_black(county_predictions, nc_stops_veil$is_dark, F)
post_diff &lt;- post_dark - post_light
county_df &lt;- as.data.frame(county_predictions)</code></pre>
<pre class="r"><code># plots of counties and difference in probability black
# between night and not night
# in the prediction dataset, odd numbers are not dark, even numbers are dark
identifying_columns &lt;- c(&#39;minute&#39;, &#39;county_name&#39;, &#39;is_dark&#39;)
county_posterior_hdi &lt;- county_predictions %&gt;%
  t() %&gt;%
  as.data.frame() %&gt;%
  bind_cols(prediction_df[identifying_columns]) %&gt;%
  pivot_longer(cols = V1:V200, 
               names_to = &#39;names&#39;, values_to = &#39;values&#39;) %&gt;%
  county_posterior_long %&gt;%
  group_by_at(identifying_columns) %&gt;%
  median_hdci(values, .width = c(.95, .5)) %&gt;%
  pivot_wider(id_cols = c(identifying_columns, &#39;values&#39;) , 
              names_from = &#39;.width&#39;, values_from = c(&#39;.lower&#39;, &#39;.upper&#39;))</code></pre>
<pre class="r"><code># plot
ggplot(county_posterior_hdi, aes(minute, values, color = is_dark)) +
  geom_line() +
  facet_wrap(~county_name)</code></pre>
<p>######################################################################################3</p>
</div>
</div>
<div id="applying-bart-to-the-veil-of-darkness-test" class="section level3">
<h3>Applying BART to the veil of darkness test</h3>
<p>We’re use the <code>bartCause</code> package to run our model. <code>bartCause</code> makes it easy to do causal inference with BART models. Although this is not a pure causal inference problem, a measurement such as the average treatment effect - with the treatment being whether it is dark - can shed light on the issue. The average treatment effect of darkness on the probability that the driver is Black represents the percentage point difference in the probability the driver is Black between daytime and darkness. This is the number we want. With <code>bartCause</code>, it’s easy to calculate this number.</p>
<p>Another benefit of using <code>bartCause</code> is that the treatment variable will be included in every tree. In a typical BART model, like in a random forest, not every variable is used in each decision tree. Including the variable signifying whether it is dark out in all trees will help create a more precise predictions about differences in whether it is dark outside.<a href="#fn4" class="footnote-ref" id="fnref4"><sup>4</sup></a></p>
<p>Here’s all the code we need to create the model. Note that the code behind this entire post is available on <a href="https://github.com/shanejorr/shanes-site/tree/main/content/post/2021-03-08-using-bayesian-additive-regression-trees-to-analyze-traffic-stop-discrimination">GitHub</a>. The confounders are additional variables we want to account for. As already noted, BART automatically incorporates non-linearities and interactions so there is no need to try non-linear terms like splines (<code>splines::ns((minute, df = 4)</code>) or interaction effects (<code>minute*county_name</code>). You get it all. And finally, we’ll use default priors for stuff like the number of trees and the number of variables per tree.</p>
<pre class="r"><code># Bart model for all counties ---------------------
bart_mod &lt;- bartc(
  response = is_black,
  treatment = is_dark,
  confounders = minute + subject_age + subject_sex + county_name,
  data = nc_stops_veil,
  estimand = &#39;ate&#39;,
  n.chains = 8
)</code></pre>
<p>The model above gives us a single percentage point difference for the 20 largest NC counties in the probability a driver is black between stops in daytime and stops at night. The code below, meanwhile, provides this probability for each of the 20 largest counties individually. It creates a different model for each county. Positive numbers tell us that there is a higher probabiity the driver pulled over is Black compared to</p>
<pre class="r"><code># Bart mdoels by county -------------------------

# initialize dataframe to store ates
bart_cty_ates &lt;- summary(bart_mod)

bart_cty_ates &lt;- overall_ate[[9]] %&gt;%
  mutate(county = &#39;All Counties&#39;)

# iterate through each county, filter for data that only includes county, run BART model
for (cty in most_pop_counties[19]) {
  
  # status update
  print(cty)
  
  # only keep needed county
  county_df &lt;- nc_stops_veil %&gt;%
    filter(county_name == !!cty)
  
  bart_mod_cty &lt;- bartc(
    response = is_black,
    treatment = is_dark,
    confounders = minute + subject_age + subject_sex,
    data = county_df,
    estimand = &#39;ate&#39;,
    n.chains = 4
  )
  
  cty_ate &lt;- summary(bart_mod_cty)

  cty_ate &lt;- cty_ate[[9]] %&gt;%
    mutate(county_name = !!cty)
  
  bart_cty_ates &lt;- bart_cty_ates %&gt;%
    bind_rows(cty_ate)
  
}</code></pre>
<p>Now for the results. The graph below shows for each county and all of North Carolina the percentage point difference between daytime and darkness in the probability that the driver pulled over is Black. The aggregate difference for the 20 largest NC counties sits near zero, with its 95% credible interval crossing zero. Functionally, there is no difference at the state level.</p>
<p>County differences bounce around zero. They are symmetrically distributed around zero and all 95% credible intervals also cross zero. This combination means that we cannot say anything with confidence about counties. We cannot even say for sure what side of zero their diffierences in probabilities sits on.</p>
</div>
</div>
<div id="logistic-regression-model-as-a-check" class="section level2">
<h2>Logistic regression model as a check</h2>
<p>As mentioned earlier, researchers often use logistic regression as the model of choice for the veil of ignorance test. We’ll sanity check the BART results with logistic regression.</p>
<p>Here’s our model. We’re only creating one aggregate model using the 20 largest counties.</p>
<pre class="r"><code># logistic regression model with LMER
nc_stops_veil &lt;- nc_stops_veil %&gt;%
  ungroup() %&gt;%
  # scale continuous predictors
  mutate(minute_scale = scale(minute),
         age_scale = scale(subject_age))

lmer_mod &lt;- glmer(
  is_black ~ is_dark + splines::ns(minute_scale, df = 4) + splines::ns(age_scale, df = 4) + subject_sex + (1 | county_name) + (1 | year),
  data = nc_stops_veil,
  family = binomial(link = &quot;logit&quot;)
)</code></pre>
<p>The following table highlights the results. The value of interest is the regression coefficient for the term signifying whether is it daytime or darkness. The coefficient is in log-odds, not the percentage point difference in probabilities. As such, the values cannot be directly compared to the estimates from the BART model. But, positive numbers have the same meaning as positive numbers in the BART model: Blacks are more likely to get pulled over during nighttime than daytime. Also like BART, zero means no effect.</p>
<p>The regression model’s result mirrors BART. The coefficient value is slightly positive, but the 95% credible interval crosses zero.
## LKimJust as in BART, the logistic regression model does not provide evidence of bias in traffic stops.</p>
<pre class="r"><code>summary(lmer_mod)$coefficients %&gt;%
  as.data.frame() %&gt;%
  mutate(ci.lower = Estimate - (`Std. Error` * 1.96),
         ci.upper = Estimate + (`Std. Error` * 1.96)) %&gt;%
  rownames_to_column() %&gt;%
  filter(rowname == &#39;is_darkTRUE&#39;) %&gt;%
  mutate(county_name = &#39;20 Largest NC Counties&#39;) %&gt;%
  select(county_name, Estimate, `Std. Error`, starts_with(&#39;ci&#39;)) %&gt;%
  mutate(across(where(is.numeric), ~round(., 3))) %&gt;%
  select(county_name, Estimate, starts_with(&#39;ci&#39;)) %&gt;%
  gt() %&gt;%
    cols_label(county_name = &quot;&quot;,
              ci.lower = &quot;95% CI lower&quot;,
              ci.upper = &quot;95% CI upper&quot;)</code></pre>
</div>
<div id="limitations-of-the-veil-of-darkness-test" class="section level2">
<h2>Limitations of the veil of darkness test</h2>
<p>The veil of darkness test, like all tests, has its limits. The first is the test’s scope. It’s not trying to discern bias writ-large, but a narrow type of bias: bias when police officers make the quick decision to pull someone over. But, there are multiple entry points of bias with law enforcement. As noted earlier, there might be racial bias is the deployment of law enforcement assets. This bias could feed into disparities in traffic stops without being picked up by the veil of darkness test.</p>
<p>But, the scope is even smaller than this. It only examines police officer bias in a small roughly three hour period when it’s daytime part of the year and nighttime part of the year.</p>
<p>Additionally, many streets contain artificial lighting. The veil of darkness test assumes that officers do not know the race of the driver they are pulling over at night. often, however, this isn’t true.</p>
<p>Finally, there are factors that correlate with race which our model doesn’t consider. Vehicle make, model, and year could correlate with race and are visible to the officer at night. If stops correlate with these three items and these items also correlate with race then we would not pick up bias.</p>
<p>That said, the test is a form of evidence pointing to bias or the lack thereof. .</p>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>Chipman, H. A., George, E. I., &amp; McCulloch, R. E. (2010). BART: Bayesian additive regression trees. The Annals of Applied Statistics, 4(1), 266-298.<a href="#fnref1" class="footnote-back">↩︎</a></p></li>
<li id="fn2"><p>Hill, J. L. (2011). Bayesian nonparametric modeling for causal inference. Journal of Computational and Graphical Statistics, 20(1), 217-240.<a href="#fnref2" class="footnote-back">↩︎</a></p></li>
<li id="fn3"><p>Pierson, E., Simoiu, C., Overgoor, J., Corbett-Davies, S., Jenson, D., Shoemaker, A., &amp; Goel, S. (2020). A large-scale analysis of racial disparities in police stops across the United States.() Nature human behaviour, 4(7), 736-745.<a href="#fnref3" class="footnote-back">↩︎</a></p></li>
<li id="fn4"><p>I learned this the hard way. I originally created the model with the <code>BART</code> package and calculated the difference in probabilities between darkness and daytime myself. But, the difference for every observation was zero. By zero, I mean literally zero, not 0.0000003 or something. This was odd. The random part of random forests also applies to BART and this randomness should prevent the difference from being exactly zero. Upon investigation, however, I saw that no trees contained the variable signifying whether it was dark outside. This explains why running predictions with changes to this variable did not alter the prediction. It’s like doubling the amount of salt in a recipe that does not call for salt. The taste will be the same. But, its exclusion from all trees does signify that the has little to no impact on predictions.<a href="#fnref4" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
