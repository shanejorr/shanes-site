---
title: Using Bayesian Additive Regression Trees to Analyze Traffic Stop Discrimination
author: ''
date: '2021-03-08'
slug: using-bayesian-additive-regression-trees-to-analyze-traffic-stop-discrimination
categories:
  - bayesian analysis
  - traffic stops
  - Bayesian Additive Regression Trees
tags: []
subtitle: ''
summary: ''
authors: []
lastmod: '2021-03-08T19:22:36-05:00'
featured: no
image:
  caption: '<span>Photo by <a href="https://unsplash.com/@gerandeklerk?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText">Geran de Klerk</a> on <a href="https://unsplash.com/s/photos/forest?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText">Unsplash</a></span>'
  focal_point: ''
  preview_only: no
projects: []
links:
- icon: github
  icon_pack: fab
  name: Github
  url: https://github.com/shanejorr/shanes-site/blob/main/content/post/2021-03-08-using-bayesian-additive-regression-trees-to-analyze-traffic-stop-discrimination/index.en.Rmd
draft: yes
---

```{r setup, echo = F}
knitr::opts_chunk$set(echo = T,
                      message = F,
                      warning = F,
                      error = F)
```

Bayesian Additive Regression Trees (BART) are a Bayesian version of random forests. They're flexible, non-parametric, and supply a full posterior distribution on outcomes. They have all the benefits of Bayesian modeling and non-parametric machine learning modeling in one algorithm.

In this post, I practice with BART models in this post using traffic stop data as the backdrop. We'll apply BART to the veil of darkness test and see if we can uncover racial disparities on traffic stops.

## Why BART?

We're not going into the technical details of BART here. Instead, check out the following resources To understand the algorithm's guts:

-   [BART: Bayesian additive regression trees](https://arxiv.org/abs/0806.3286) - The paper that introduced the world to BART.^[Chipman, H. A., George, E. I., & McCulloch, R. E. (2010). BART: Bayesian additive regression trees. The Annals of Applied Statistics, 4(1), 266-298.]
-   [Bayesian Nonparametric Modeling for Causal Inference](https://www.tandfonline.com/doi/abs/10.1198/jcgs.2010.08162) - Jennifer Hill's paper that brought us BART for causal inference.^[Hill, J. L. (2011). Bayesian nonparametric modeling for causal inference. Journal of Computational and Graphical Statistics, 20(1), 217-240.]
-   [Causal Inference at the Intersection of Statistics and Machine Learning](https://www.youtube.com/watch?v=mJVTg8PWjW4) - Video presentation by Jennifer Hill on BART fo causal inference. The video isn't great quality, but it's an easy to follow introduction to BART.
-   [Introduction to Bayesian Additive Regression Trees (BART) for Causal Inference](https://www.youtube.com/watch?v=9d5-3_7u5a4) - Another good video introduction to BART for causal inference.

In general terms, however, I view the pros of BART as falling into two categories: (1) reasons it's better than parametric Bayesian models such as linear or logistic regression, and (2) reasons it's better than other non-parametric methods like random forests. Let's look at these two categories.

### Why BART is (often) better than parametric Bayesian methods

Compared to parametric models, BART lets you easily and automatically discover a prediction model's best functional form. The best, at least, in terms of partitioning the feature space as trees. In a way, it's plug and play. 

There are a few reasons why this brings benefits over parametric Bayesian models. First, no more worrying about the assumption of linearity between the predictors and the outcome. No more fishing around with splines or polynomials just in case the relationship is nonlinear. If it's nonlinear, the algorithm will create a nonlinear fit without you intervening.

For causal inference, the algorithm automatically incorporates interactions to the degree they appear in the data. No more trying multiple models, some with and some without interactions, to see if there are interaction effects. You get them for free.

Finally, variable selection is simplified. Predictors that do not predict the outcome will simply not find their way into any trees. Humans don't have to identify and exclude them, the model identifies and excludes them.

### Why BART is (often) better than other non-parametric methods

There's one answer here: you get the full posterior distribution of your outcome just like with any Bayesian model. This is crucial with causal inference as it let's you map the uncertainty of your treatment effects. Black-box machine learning models don't do this. They give you one answer and tell you little about uncertainty in the answer.

## What is the veil of darkness test?

This post is about me learning and applying BART. But, applying algorithms I'm interested in to topics I'm curious about helps me stay engaged. This post's topic is racial disparities in traffic stops using the veil of darkness test.

Here's how [a group of scholars](https://www.nature.com/articles/s41562-020-0858-1) explained the test:

>[The] method starts from the idea that officers who engage in racial profiling are less able to identify a driver’s race after dark than during the day. As a result, if officers are discriminating against black drivers—all else being equal—one would expect black drivers to comprise a smaller share of stopped drivers at night, when a veil-of-darkness masks their race. To account for patterns of driving and police deployment that may vary throughout the day, the test leverages the fact that the sun sets at different times during the year.^[Pierson, E., Simoiu, C., Overgoor, J., Corbett-Davies, S., Jenson, D., Shoemaker, A., & Goel, S. (2020). A large-scale analysis of racial disparities in police stops across the United States.() Nature human behaviour, 4(7), 736-745.]

For example, let's say it is dark at 7 p.m. for a third of the year, light for a third of the year, and kind-of dark for a third of the year. We want to compare the percentage of all drivers pulled over are Black and when it's light at 7 p.m. against the percentage of all drivers are pulled over who are Black when it's dark at 7 p.m. If a larger percentage are pulled over when it's light we have some evidence of racial discrimination.  

This reduces to a prediction problem. For each traffic stop, we want to predict the driver's race given the time of the stop, the location (county), whether it's dark out, and other possible covariates. Then, we'll turn around and predict every driver's race. If the probability that a driver is Black is higher during the day than at night - holding time, location, and other covariates constant - then we have evidence of racial discrimination. 

Researchers generally apply logistic regression to this problem. And that's a great tool. But, we're here to learn BART, so that what we're applying. Plus, BART provides all the benefits over parametric models that we mentioned earlier. 

## Exploratory analysis of racial disparities

Talk about data.

```{r importLibraries}
library(glue)
library(vroom)
library(modelr)
library(tidycensus)
library(lubridate)
library(highcharter)
library(scales)
library(widgetframe) 
library(suncalc)
library(tigris)
library(sf)
library(bartCause)
library(tidybayes)
library(lme4)
library(tidyverse)
```

```{r customFunctions}
# custom functions --------------------------

# create ggplot theme for this post
post_theme <- theme_minimal() +
  theme(legend.position = 'bottom',
        plot.title = element_text(size = 11),
        plot.subtitle = element_text(size = 10),
        axis.title = element_text(size = 10))

theme_set(post_theme)

# standard credits to all to all high charts plots

# credits to be used on all plots
add_credits <- function(hc) {
  hc %>%
    hc_credits(
      enabled = TRUE,
      text = "Data source: The Stanford Open Policing Project | US Census Bureau Pop. Estimates",
      href = FALSE
  )
}

scatter_stops_per <- function(df, second_race, plot_title, y_title) {
  
  # scatter plot of stops per 100 residents for white and black / hispanic drivers
  # second_race is the race to compare with white
  
  # max axis point
  max_point <- 45
  
  # find out if white or minority group is higher, used for colors above or below line
  df[['diff']] <- ifelse(df[['white']] < df[[second_race]], 1, 0)
  
  # assign color to each row
  color_palette <- c('#4e79a7', '#f28e2b')
  df[['colors']] <- colorize(df[['diff']], color_palette)
  
  # tooltip
  scatter_tooltip <- "<b>{point.county_name}</b><br>
                      Stops per 100 residents (Black): <b>{point.black}</b><br>
                      Stops per 100 residents (White): <b>{point.white}</b><br>
                      Stops per 100 residents (Hispanic): <b>{point.hispanic}</b><br>
                      County population (2013): <b>{point.value_tooltip}</b><br>"
  
  df %>%
    hchart(
      "bubble", 
      hcaes(x = 'white', y = .data[[second_race]], size = value, color = colors)
    ) %>%
    hc_add_series(name = 'line',
                  data = data.frame(
                    x = seq(1, max_point, 1),
                    y = seq(1, max_point, 1),
                    type = 'line',
                    marker = list(enabled = FALSE)
                  )) %>%
    # remove tooltip for line
    hc_plotOptions(line = list(enableMouseTracking = FALSE),
                   bubble = list(opacity = .85,
                                 minSize = "1%", maxSize = "7%")) %>%
    hc_xAxis(min = 1, max = max_point, type = 'logarithmic',
             title = list(text = 'Stops per 100 residents for White drivers')) %>% 
    hc_yAxis(min = 1, max = max_point, type = 'logarithmic',
             title = list(text = y_title)) %>%
    hc_tooltip(
        headerFormat = "",
        pointFormat = scatter_tooltip
    ) %>%
    hc_title(text = plot_title)
}

# extract time from date time
time_to_minute <- function(time) {
  # minutes since midnight
  hour(time) * 60 + minute(time)
}

create_train_test_sets <- function(split_partition, predictor_variables, train_or_test = 'train') {
  
  # create train and testing datasets
  
  if (train_or_test == 'train') {
    
    pred_df <- training(split_partition)
    
  } else if (train_or_test == 'test') {
    
    pred_df <- testing(split_partition)
    
  } else {
    stop("train_or_test must be either 'train' or 'test'")
  }
  
  pred_df %>%
      ungroup() %>%
      # only keep predictors (required for BART)
      select(!!predictor_variables) %>%
      as.data.frame()
  
}
```

## Stops per 100 residents for black and white drivers

```{r importData, cache = T}
# downlaod traffic stop data --------------------------

# download entire state traffic stop dataset
nc_stops <- vroom('https://shane-datasets.nyc3.digitaloceanspaces.com/traffic-stop/nc/nc_statewide_2020_04_01.csv.gz',
                  col_select = -contains('raw'))

# get 2010 - 2013 5 year and aggregate
pop_years <- seq(2009, 2013, 1)

n_years <- length(pop_years)

nc_stops <- nc_stops %>%
  mutate(year = year(date)) %>%
  # only keep between years 2010 and 2013 
  # 2013 is final full year of stop data
  filter(year %in% !!pop_years)
```

## Comparing Stops Per 100 Residents Among Racial Groups

```{r importCensus, cache = T}
# racial populations by county from census ----------------

# recode integers for dates to years
recode_date <- c(
  `3` = 2010,
  `4` = 2011,
  `5` = 2012,
  `6` = 2013,
  `7` = 2014,
  `8` = 2015
)

# get 2013 overall county populations
county_pop <- get_estimates(geography = "county", state = 'NC',
                            product = 'population', 
                            time_series = T,
                            year = 2018) %>%
  # only keep 2013 and only keep population (not density)
  filter(DATE == 6,
         variable == 'POP') %>%
  mutate(NAME = str_remove_all(NAME, ' County, North Carolina')) %>%
  select(NAME, value)

# import population data by county and race
county_pop_race <- get_estimates(geography = "county", state = 'NC',
                                product = 'characteristics', 
                                breakdown = c('RACE', 'HISP'),
                                breakdown_labels = T,
                                time_series = T,
                                year = 2018) 

# calculate aggregate percentage of population by race
county_pop_agg <- county_pop_race %>%
  # only keep 2010 - 2013
  filter(between(DATE, 3, 6)) %>%
  # aggregate population by race for each county
  group_by(GEOID, NAME, RACE, HISP) %>%
  summarize(agg_pop = sum(value)) %>%
  ungroup() %>%
  filter(HISP != 'Both Hispanic Origins') %>%
  # make the All Races  race that is Hisp the Hisp race
  mutate(RACE = ifelse(RACE == 'All races' & HISP == 'Hispanic', 'Hispanic', RACE)) %>%
  # only need white, black, and hispanic
  filter(RACE %in% c('White alone', 'Black alone', 'Hispanic'),
         # do not need hispanic breakdown by race
         RACE == 'Hispanic' | HISP == 'Non-Hispanic') %>%
  # remove 'alone' phrase from race
  mutate(RACE = str_remove_all(RACE, ' alone'),
         # make lower case and remove latino to match traffic stops
         RACE = str_to_lower(RACE),
         # remove string so that it matches with traffic stop data
         NAME = str_remove_all(NAME, ' County, North Carolina')) %>%
  # don't need the hispanic column anymore
  select(-HISP)
```

```{r stopsByRace}
# calculate stops per race --------------

# save list of stop reasons to be uses in text
stop_reasons <- unique(nc_stops$reason_for_stop)

stops_by_race <- nc_stops %>%
  # calculate number of stops by race (numerator)
  group_by(county_name, subject_race) %>%
  summarize(num_stops_race = n()) %>%
  # calculate percentage of stops by race
  ungroup() %>%
  # stops per 100 people
  #mutate(stops_per = (num_stops_race*100) / agg_pop) %>%
  # only need three races; not enough data for others
  filter(subject_race %in% c('black', 'white', 'hispanic')) %>%
  # only keep counties with a minimum of 100 stops for each race
  group_by(county_name) %>%
  mutate(min_num = min(num_stops_race)) %>%
  ungroup() %>%
  filter(min_num >= 500) %>%
  drop_na(county_name) %>%
  mutate(county_name = str_remove_all(county_name, ' County'),
         # correct spelling mistake
         county_name = str_replace(county_name, 'Tyrell', 'Tyrrell')) %>%
  drop_na(county_name) %>%
  # combine stops by race percentages and racial population percentages
  left_join(county_pop_agg, 
            by = c('county_name' = 'NAME', 'subject_race' = 'RACE')) %>%
  # number of stops per 100 people
  mutate(stops_per = round(num_stops_race / (agg_pop / 100), 2)) %>%
  # convert to wide form where each race is in a different column
  # needed for plotting
  pivot_wider(id_cols = 'county_name', 
              names_from = 'subject_race',
              values_from = "stops_per") %>%
  # combine county populations
  left_join(county_pop,
            by = c('county_name' = 'NAME')) %>%
  mutate(value_tooltip = number(value, accuracy = 1, big.mark = ',')) %>%
  # Alleghany is odd outlier, so remove
  filter(!county_name %in% c('Alleghany'))

```

```{r plotStopsRace}
# scatter plot of race and perc. stops compared to overall perc. pop ----------


bw_stops_title <- 'Stops Per 100 Residents for Black and White Drivers'
bw_y_title <- 'Stops per 100 residents for Black drivers'
bw_scatter_plot <- scatter_stops_per(stops_by_race, 'black', bw_stops_title, bw_y_title)

hw_stops_title <- 'Stops Per 100 Residents for Hispanic and White Drivers'
hw_y_title <- 'Stops per 100 residents for Hispanic drivers'
hw_scatter_plot <- scatter_stops_per(stops_by_race, 'hispanic', hw_stops_title, hw_y_title) %>%
  add_credits()

hw_grid(
  bw_scatter_plot, hw_scatter_plot,
  ncol = 2, rowheight = 300
)
```

## Veil Of Darkenss Test

```{r minutesToDuskDataset}
# create dataset for veil of darkness test -----------------------

# get centroid of every county

# get county shapefiles and centroids
nc_counties <- counties(state = 'NC', cb = T)

# find centroid of each county by using it's boundaries
nc_counties$lon<-st_coordinates(st_centroid(nc_counties))[,1]
nc_counties$lat<-st_coordinates(st_centroid(nc_counties))[,2]

# drop shapefile column
nc_counties$geometry <- NULL

nc_counties <- nc_counties %>%
  ungroup() %>%
  select(GEOID, NAME, lon, lat)

# get sunlight times for each county

# extend dataset to include a county row for each day
# neede because function to calcualte sunset requires date column in dataframe
days_seq <- seq(as.Date(min(nc_stops$date)), as.Date(max(nc_stops$date)), "days")

nc_counties_sunset <- map_df(days_seq, function(day) {
  nc_counties %>%
    mutate(date = !!day) %>%
    getSunlightTimes(
      data = .,
      keep = c("sunset", "dusk"), 
      tz = 'EST'
    )
}) %>%
  mutate(
    sunset_minute = time_to_minute(sunset),
    dusk_minute = time_to_minute(dusk),
    date = ymd(str_sub(date, 1, 10))
) %>% 
  # join county names
  left_join(nc_counties, by = c('lon', 'lat'))

# only keep the 20 most populated counties
most_pop_counties <- county_pop %>%
  arrange(desc(value)) %>%
  head(20) %>%
  .[[1]]

# merge dusk times with nc stops dataset
nc_stops_veil <- nc_stops %>%
  drop_na(time) %>%
  mutate(county_name = str_remove_all(county_name, ' County')) %>%
  # only keep the 20 most populated counties
  filter(county_name %in% !!most_pop_counties) %>%
  # add sunset and dusk times
  left_join(nc_counties_sunset, 
            by = c('county_name' = 'NAME', 'date')) %>%
  # convert date times to integer minutes from midnight
  mutate(
    minute = time_to_minute(time),
    minutes_after_dark = minute - dusk_minute,
    is_dark = minute > dusk_minute
  ) %>%
  ungroup() %>%
  group_by(county_name) %>%
  # find the min and max dusk times for each county
  mutate(
    min_dusk_minute = min(dusk_minute),
    max_dusk_minute = max(dusk_minute),
    is_black = subject_race == "black"
  ) %>% 
  filter(
    # Filter to get only the intertwilight period
    minute >= min_dusk_minute,
    minute <= max_dusk_minute,
    # Remove ambigous period between sunset and dusk
    !(minute > sunset_minute & minute < dusk_minute),
    # Compare only white and black drivers
    subject_race %in% c("black", "white")
  ) %>%
  select(date, time, sunset, dusk, contains('minute'), is_dark, everything()) %>%
  drop_na(is_black, is_dark, minute, subject_age, subject_sex, county_name) %>%
  mutate(county_name = factor(county_name),
         year = as.factor(year))

# remove items to save RAM
rm(county_pop_race)
rm(nc_stops)
rm(nc_counties_sunset)
gc()
```

## BART Causal Model

```{r bartDataset, eval = F}
# Bart model for all counties ---------------------
bart_mod <- bartc(
  response = is_black,
  treatment = is_dark,
  confounders = minute + subject_age + subject_sex + county_name,
  data = nc_stops_veil,
  estimand = 'ate',
  n.chains = 8
)
```

```{r bartCtyAte, eval = F}
# Bart mdoels by county -------------------------

# initialize dataframe to store ates
bart_cty_ates <- summary(bart_mod)

bart_cty_ates <- overall_ate[[9]] %>%
  mutate(county = 'All Counties')

# iterate through each county, filter for data that only includes county, run BART model
for (cty in most_pop_counties) {
  
  # status update
  print(cty)
  
  # only keep needed county
  county_df <- nc_stops_veil %>%
    filter(county_name == !!cty)
  
  bart_mod_cty <- bartc(
    response = is_black,
    treatment = is_dark,
    confounders = minute + subject_age + subject_sex,
    data = county_df,
    estimand = 'ate',
    n.chains = 4
  )
  
  cty_ate <- summary(bart_mod_cty)

  cty_ate <- cty_ate[[9]] %>%
    mutate(county_name = !!cty)
  
  print(cty_ate)
  
  bart_cty_ates <- bart_cty_ates %>%
    bind_rows(cty_ate)
  
  print(bart_cty_ates)
  
  write_csv(bart_cty_ates, 'bart_cty_ates.csv')
  
  rm(bart_mod_cty)
  
}
```

## Logistic regression model as a check

```{r}
# logistic regression model with LMER
nc_stops_veil <- nc_stops_veil %>%
  ungroup() %>%
  # scale continuous predictors
  mutate(minute_scale = scale(minute),
         age_scale = scale(subject_age))

lmer_mod <- glmer(
  is_black ~ is_dark + minute_scale + age_scale + subject_sex + (1 | county_name) + (1 | year),
  data = nc_stops_veil,
  family = binomial(link = "logit")
)

names(summary(lmer_mod))

summary(lmer_mod)$coefficients %>%
  as.data.frame() %>%
  mutate(ci.lower = Estimate - (`Std. Error` * 1.96),
         ci.upper = Estimate + (`Std. Error` * 1.96)) %>%
  rownames_to_column() %>%
  filter(rowname == 'is_darkTRUE') %>%
  mutate(county_name = '20 Largest NC Counties') %>%
  select(county_name, Estimate, `Std. Error`, starts_with('ci')) %>%
  mutate(across(where(is.numeric), ~round(., 3)))
```
